from flask import Flask, request, jsonify
from flask_cors import CORS  
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

app = Flask(__name__)
CORS(app, resources={r"/detect_text": {"origins": "*"}})   # ← add
# 1-line “mini-model”: HuggingFace user-contributed classifier
MODEL_NAME = "roberta-base-openai-detector"   # Community model
tok = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

@torch.inference_mode()
def predict_prob(text: str) -> float:
    tokens = tok(text, truncation=True, max_length=512, return_tensors="pt")
    logits = model(**tokens).logits
    # model outputs [P(human), P(AI)]; take softmax then AI prob
    prob_ai = torch.softmax(logits, dim=-1)[0, 1].item()
    return prob_ai

@app.route("/detect_text", methods=["POST"])
def detect_text():
    text = request.get_json(force=True)["text"]
    prob_ai = predict_prob(text)
    print("prob ai:", prob_ai)
    return jsonify({"probability": prob_ai})

if __name__ == "__main__":
    # --- one-line sanity check -------------------------------------------
    print("Smoke-test – AI probability:",
          predict_prob("This sentence was generated by an AI language model."))
    # ---------------------------------------------------------------------

    # Allow cross-origin JS calls from the content script
    app.run(host="127.0.0.1", port=5000, debug=True)